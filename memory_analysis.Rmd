---
title: "R Notebook"
output: html_notebook
---


**INTRODUCTION**

For this report, the relationship for cost of computer memory over time is investigated. A dataset of memory price per megabyte at different time of the year from 1957 to 2018 is provided through the url: https://jcmit.net/memoryprice.htm. Through an analysis of the dataset, the goal of this investigation is to examine patterns of fluctuation in the cost of computer memory, and with the result, reasonably predict future costs.

Before the analysis begins, it is identified that there are two possible methods to read the dataset. The first way is to extract usable data through the given Excel file, as provided in the link. This method requires some tidying to separate the same file into multiple tables. The second way is to directly scrape the table off the HTML site, referencing the table element during the process. An original attempt to read straight from the web, but a couple of rows were missing and the whole process was complicated. Thus, the approach to read directly from the .xls file was chosen for the purpose of this report.

The computer memory data required for the purpose of this analysis is in the MEMORY sheet. There are 14 columns and 359 rows. The headers take up 4 rows, so there should really be 355 rows of data with one row for the header. The following is a brief summary of the columns:

  1. The fractional (decimal) year the entry was recorded
  2. Cost per megabyte
  3. The year the entry was recorded
  4. The month the entry was recorded
  5. The type of the catalogue where the memory unit was sold
  6. The page where the advert can be found in the catalogue
  7. The store where the memory unit was sold
  8. The size of the memory unit in kilobytes
  9. The cost (in USD) of the memory unit
  10. Timings (speed) on the memory unit
  11. Type of memory the unit uses
  12. The brand of the memory unit. There appears to be comments and a bunch of other data types in this column
  13. What seems to be an outdated price column. Some of the prices are also defined along the memory type, which all match with column 9 but not this column.
  14. What seems to be an outdated timings (speed) column. Again, there seems to be more accurate data in column 10, and the data in column 10 seems to be more complete.

To ensure the flow of this report, the report will consist of a combination of R code and explanations. Then begin the extraction of the data, where the first step is to load in dependencies:
```{r warning=FALSE, message=FALSE}

require("gdata")
require("tidyverse")

```

When it is ready, the data is read. There are a few issues to note with the data frame that's been read in:

1. The data frame has 16 columns, whilst the needed data only fills 14 of the columns. It looks like two empty columns were read in at the end. The resolution is just to remove them.

2. There were 4 rows for the header in the data, but the dataframe took the first row as the header and the rest as data. A step is needed to remove the first three rows in the data.

3. The headers poorly represent the data, and new headers will need to be defined.

```{r warning=FALSE, message=FALSE}

#Read data
memory = read.xls("MemDiskPrice-xl95.xls", sheet = 1, header = TRUE)

#Display data
memory

```

**METHODS**
After reading the data into a dataframe, each row is indexed. The indices are out of order since some rows are removed, and needs an update:
```{r warning=FALSE, message=FALSE}

#Remove the last 2 columns
memory = subset(memory, select=-c(`X.11`, `X.12`))

#Remove the first 3 rows
memory = tail(memory, -3)

#Re-define column names
colnames(memory) = c("frac_year", "cost_per_Mb", "year", "month", "cat_type", "page", "store", "KB", "unit_cost", "clock", "mem_type", "brand", "price_outdated", "clock_outdated")

#Remove row names (indexes)
rownames(memory) = NULL

#Display data
memory

```

There are still some unneeded columns in the dataframe. It is observed that the last two outdated columns can be removed since they contain outdated data. Fractional year already contains all information conveyed by year and month, and is redundant. The page column contains the page number in a particular magazine, where information on the storage device can be found. Along with the brand column, they won't be much use to the analysis of memory prices through time, and can be disregarded for the purpose of this analysis. At last, numerical columns are converted from factor to numericals.
```{r warning=FALSE, message=FALSE}

#Remove the outdated columns
memory = memory[-14]
memory = memory[-13]

#Remove year and month
memory = subset(memory, select=-c(year, month))

#Remove catalogue, store, and page data
memory = subset(memory, select=-c(cat_type, page, store, brand))

#Convert to numeric
memory$frac_year = as.numeric(gsub(",", "", memory$frac_year))
memory$cost_per_Mb = as.numeric(gsub(",", "", memory$cost_per_Mb))
memory$KB = as.numeric(gsub(",", "", memory$KB))
memory$unit_cost = as.numeric(gsub(",", "", memory$unit_cost))

memory

```


**ANALYSIS**

We first plotted cost per Mb against year, but the plot followed a rational pattern. We tried to assign a linear model to it, but it had an abysmally small R-squared adjusted value of 0.04. This means that the linear model predicts very poorly. However, we've noticed that the p-value was extremely small, which meant that a linear relationship did exist. We've realized that, since the plot followed a rational (1/x) pattern, we could try to take the logarithm of the price per Mb variable, which could normalize the data. When we've taken the logarithm on cost per Mb and replotted the data, a very obvious linear trend appeared. Running the linear model on the normalized data, we get an R-squared adjusted value of 0.9743, and an even more significant p-value. This suggests that the linear model fits extremely well for this data.
```{r warning=FALSE, message=FALSE}

#Plot fractional year against cost per Mb
ggplot(data=memory, aes(x=frac_year, y=cost_per_Mb)) + 
  geom_point() +
  scale_x_continuous(breaks = round(seq(min(memory$frac_year), max(memory$frac_year), by = 5),1)) +
  scale_y_log10(labels = function(x) format(x, big.mark = ",", scientific = FALSE))

#Fit a linear model
memory2 = lm(log(cost_per_Mb)~frac_year,data=memory)
summary(memory2)

```


We can see below that the differences in cost per Mb between each year is also linear.
```{r warning=FALSE, message=FALSE}

#Calculate differences in cost per Mb every year
cost_diff = memory[-nrow(memory),]$cost_per_Mb
cost_diff_subtract = memory[-1,]$cost_per_Mb

#Add NA as first entry as we do not know what the cost difference is between the first year in our data and the year before
cost_diff = cost_diff_subtract - cost_diff
cost_diff = append(cost_diff, NA, after = 0)

#Add difference column to data
memory$cost_diff = cost_diff

#Graph the cost differences against time
ggplot(data=memory, aes(x=frac_year, y=cost_diff)) + 
  geom_point() +
  scale_x_continuous(breaks = round(seq(min(memory$frac_year), max(memory$frac_year), by = 5),1)) +
  scale_y_log10(labels = function(x) format(x, big.mark = ",", scientific = FALSE))

```
Again, using the normalized data in this situation, we run a t-test with the null hypothesis that the difference in means is equal to 0. From the p-value taken from this test, we get 2.2e-16. This is obviously much less than 0.05, so we can reject the null hypothesis in favour of the alternative hypothesis. This further strengthens the claim that we've been seeing that the price has indeed been changing as the years go by.

```{r}
t.test(log(memory$cost_per_Mb),memory$frac_year,data=memory,alternative="less")
```

We also chose to carry out an ANOVA test on the same null hypothesis for this data. We will use this test to also see if the means have differed over the years. In this test, we can see that the p-value was also extremely low, being 3.9e-05. This is also less than 0.05, so we also reject the null hypothesis in favour of the alternative hypothesis. 
```{r}
memory.aov = aov(frac_year~cost_per_Mb,data=memory)
summary(memory.aov)
```

Both these tests further support the claim taken from the plots, showing us that the cost of memory has in fact changed over time.

[then ... add predictions with the found model]
 

